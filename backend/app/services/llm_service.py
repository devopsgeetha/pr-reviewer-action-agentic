"""
Service for LLM integration using Langchain and OpenAI
"""
import os
from typing import Dict, Any, Optional
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate


class LLMService:
    """Handle LLM operations for code analysis"""

    def __init__(self, rag_service=None):
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.model = os.getenv("OPENAI_MODEL", "gpt-4o")
        self.temperature = float(os.getenv("OPENAI_TEMPERATURE", 0.3))
        self.rag_service = rag_service  # Optional RAG service
        
        # Debug logging
        print(f"LLMService init - API Key present: {bool(self.api_key)}")
        if self.api_key:
            print(f"LLMService init - API Key starts with: {self.api_key[:10]}...")

        # Only initialize LLM if API key is available
        if self.api_key:
            try:
                self.llm = ChatOpenAI(
                    model=self.model,
                    temperature=self.temperature,
                    api_key=self.api_key,  # Use api_key parameter instead of openai_api_key
                    max_tokens=int(os.getenv('MAX_TOKENS', 8000)),
                )
                print(f"LLMService init - Successfully initialized ChatOpenAI with model {self.model}")
            except TypeError as exc:
                # Some environments configure unsupported kwargs (e.g. proxies).
                print(
                    "Warning: Failed to initialize OpenAI client. "
                    f"Using fallback mode. Details: {exc}"
                )
                self.llm = None
            except Exception as exc:
                print(
                    "Warning: Unexpected error initializing OpenAI client. "
                    f"Using fallback mode. Details: {exc}"
                )
                self.llm = None
        else:
            self.llm = None
            print("Warning: OPENAI_API_KEY not set. LLM features will be disabled.")

        self._init_prompts()

    def _init_prompts(self):
        """Initialize prompt templates"""

        # Code analysis prompt
        self.analysis_prompt = PromptTemplate(
            input_variables=["code", "filename", "language"],
            template="""You are a senior staff engineer performing an in-depth code review.
Provide a COMPREHENSIVE, DETAILED analysis of the following code changes.

Filename: {filename}
Language: {language}

Code Changes (Git Diff Format):
```
{code}
```

**IMPORTANT**: The above is a git diff. Lines starting with + are additions, lines with - are deletions.
Focus your analysis on the + lines (new/changed code). For line numbers:
- If you see "@@ -X,Y +A,B @@", the new lines start at line A
- Count the + lines from that starting point to determine specific line numbers
- Only report line numbers for + lines (new/added code)

Perform a THOROUGH analysis covering:
1. **Bugs and Errors**: Find ALL potential bugs, logic errors, runtime issues, edge cases, race conditions
2. **Security Vulnerabilities**: Deep security analysis - injection risks, auth flaws, data exposure, unsafe practices
3. **Code Quality**: Architecture, design patterns, readability, maintainability, best practices, code smells
4. **Performance**: Bottlenecks, inefficiencies, optimization opportunities, algorithmic complexity
5. **Style and Standards**: Coding standards, conventions, documentation, naming, structure
6. **Testing**: Test coverage implications, testability, missing test cases
7. **Error Handling**: Exception handling, error cases, input validation, edge cases
8. **Architecture**: Design decisions, patterns, SOLID principles, separation of concerns

Provide your response ONLY as a valid JSON object in this exact format (NO markdown, NO code blocks, NO additional text):
{{
    "issues": [
        {{
            "severity": "high|medium|low",
            "category": "bug|security|quality|performance|style|testing|architecture",
            "message": "DETAILED description of the issue with specific reasoning and impact",
            "line": actual_line_number_in_new_file,
            "file": "{filename}",
            "suggestion": "SPECIFIC, ACTIONABLE recommendation with code examples if possible",
            "reasoning": "WHY this is an issue and WHAT could go wrong",
            "impact": "Potential consequences if not addressed"
        }}
    ],
    "suggestions": [
        "Detailed improvement suggestions with specific examples and rationale"
    ]
}}

CRITICAL REQUIREMENTS:
- Return ONLY the JSON object - no explanatory text before or after
- Do NOT wrap the JSON in markdown code blocks (no ``` characters)
- Do NOT include any text outside the JSON structure
- Find AT LEAST 3-5 meaningful issues per file (be thorough, not superficial)
- Be EXTREMELY detailed in messages - explain the WHY and HOW
- **MANDATORY**: Every issue MUST have both "line" and "file" fields filled
- For "line": Count from the @@ +A,B @@ marker - A is starting line, count + lines from there
- For "file": Always use exactly: {filename}
- If you can't determine exact line numbers, use your best estimate from the + lines
- Provide CONCRETE, ACTIONABLE suggestions with code examples
- Include reasoning and impact for each issue
- Prioritize issues properly: high (critical bugs/security), medium (quality/performance), low (style/minor)
- Look for SUBTLE issues, not just obvious ones
- Consider edge cases, error handling, and maintainability

EXAMPLE LINE COUNTING:
If you see "@@ -10,5 +10,8 @@", new lines start at line 10.
If there are 3 + lines, they would be approximately lines 10, 11, 12.

DO NOT BE SUPERFICIAL. This is a professional code review requiring deep analysis.""",
        )

        # Summary generation prompt
        self.summary_prompt = PromptTemplate(
            input_variables=["context", "issues_count", "suggestions_count"],
            template="""Generate a concise summary for a pull request review.

PR Context:
- Title: {context}
- Total Issues Found: {issues_count}
- Total Suggestions: {suggestions_count}

Provide a brief 2-3 sentence summary that:
1. Highlights the main findings
2. Mentions critical issues if any
3. Gives an overall assessment

Keep it professional and constructive.""",
        )

    def analyze_code_changes(
        self, code: str, filename: str, language: str
    ) -> Dict[str, Any]:
        """
        Analyze code changes using LLM with optional RAG context

        Args:
            code: The code diff or snippet
            filename: Name of the file
            language: Programming language

        Returns:
            Dictionary containing analysis results
        """
        if not self.llm:
            return {
                "issues": [],
                "suggestions": [
                    "OpenAI API key not configured. LLM analysis unavailable."
                ],
            }

        try:
            # Get RAG context if available
            rag_context = ""
            if self.rag_service:
                try:
                    rag_context = self.rag_service.get_relevant_context(
                        code, filename, language
                    )
                except Exception as e:
                    print(f"Warning: RAG context retrieval failed: {str(e)}")

            # Build enhanced prompt with RAG context
            enhanced_code = code
            if rag_context:
                enhanced_code = f"""# Relevant Context from Past Reviews:
{rag_context}

# Code to Review:
{code}"""

            # Use new LangChain pattern (RunnableSequence) instead of deprecated LLMChain
            # Format prompt with variables and use pipe operator
            formatted_prompt = self.analysis_prompt.format(
                code=enhanced_code, filename=filename, language=language
            )
            
            # Use pipe operator: prompt | llm (replaces deprecated chain.run)
            from langchain_core.messages import HumanMessage
            message = HumanMessage(content=formatted_prompt)
            result = self.llm.invoke([message]).content

            # Parse result (assuming JSON response)
            import json
            import re

            try:
                # Try parsing as-is first
                analysis = json.loads(result)
            except json.JSONDecodeError:
                # Try extracting JSON from markdown code blocks
                try:
                    # Look for JSON in code blocks: ```json ... ``` or ``` ... ```
                    json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', result, re.DOTALL)
                    if json_match:
                        analysis = json.loads(json_match.group(1))
                    else:
                        # Try finding raw JSON object
                        json_match = re.search(r'(\{.*\})', result, re.DOTALL)
                        if json_match:
                            analysis = json.loads(json_match.group(1))
                        else:
                            # Last resort: treat as plain text suggestion
                            analysis = {"issues": [], "suggestions": [result]}
                except (json.JSONDecodeError, AttributeError):
                    # If all parsing fails, return as plain text
                    analysis = {"issues": [], "suggestions": [result]}

            return analysis

        except Exception as e:
            print(f"Error in LLM analysis: {str(e)}")
            return {"issues": [], "suggestions": []}

    def generate_summary(self, context: Dict, review_result: Dict) -> str:
        """
        Generate summary of the review

        Args:
            context: Context about the PR/MR
            review_result: Results from code analysis

        Returns:
            Summary text
        """
        if not self.llm:
            return "Code review completed. OpenAI API key not configured for detailed summary."

        try:
            # Use new LangChain pattern instead of deprecated LLMChain
            formatted_prompt = self.summary_prompt.format(
                context=str(context),
                issues_count=len(review_result.get("issues", [])),
                suggestions_count=len(review_result.get("suggestions", [])),
            )
            
            # Use pipe operator: prompt | llm (replaces deprecated chain.run)
            from langchain_core.messages import HumanMessage
            message = HumanMessage(content=formatted_prompt)
            summary = self.llm.invoke([message]).content

            return summary.strip()

        except Exception as e:
            print(f"Error generating summary: {str(e)}")
            return "Code review completed. Please review the detailed findings below."
